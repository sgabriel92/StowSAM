{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm, trange\n",
    "from skimage import transform, io, segmentation\n",
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% set up the parser\n",
    "parser = argparse.ArgumentParser(description=\"preprocess RGB images\")\n",
    "# raw input (images, masks)\n",
    "# parser.add_argument(\"--img_folder\", type=str, default=\"data/StowSam/raw_input/train/images\", help=\"path to a folder of images\")\n",
    "# parser.add_argument(\"--mask_folder\", type=str, default=\"data/StowSam/raw_input/train/masks\", help=\"path to a folder of masks (ground truth)\")\n",
    "# parser.add_argument(\"--h5_file\", type=str, default=\"dataset/bin_syn/train_shard_000000_copy\", help=\"path to a h5 file of rgb_images, masks, depths_images, metadatas\")\n",
    "parser.add_argument(\"--h5_file\", type=str, default=\"dataset/bin_syn/train_shard_000000\", help=\"path to a h5 file of rgb_images, masks, depths_images, metadatas\")\n",
    "\n",
    "# model input\n",
    "parser.add_argument(\"--data_folder\", type=str, default=\"dataset/StowSam/input/train\", help=\"path to save npz files (input for training)\")\n",
    "parser.add_argument(\"--data_name\", type=str, default=\"stow\", help=\"dataset name; used to name the final npz file, e.g., stow.npz\")\n",
    "# image parameters\n",
    "parser.add_argument(\"--img_size\", type=int, default=256, help=\"image size\")\n",
    "parser.add_argument(\"--img_format\", type=str, default='png', help=\"image format\")\n",
    "# SAM model parameters\n",
    "parser.add_argument(\"--model_type\", type=str, default=\"vit_b\", help=\"model type\")\n",
    "parser.add_argument(\"--checkpoint\", type=str, default=\"work_dir/MedSAM/sam_vit_b_01ec64.pth\", help=\"checkpoint\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda:0\", help=\"device\")\n",
    "# misc\n",
    "parser.add_argument(\"--seed\", type=int, default=2023, help=\"random seed\")\n",
    "# parse the arguments\n",
    "args = parser.parse_known_args()[0]\n",
    "# print(f\"args: {args}\")\n",
    "\n",
    "join = os.path.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% set up the model\n",
    "sam_model = sam_model_registry[args.model_type](checkpoint=args.checkpoint).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_h5file(filename: str, num_samples: int = 1000, chunk_id: int = 0):\n",
    "    with h5py.File(f\"{filename}.h5\", 'r') as h5_file:\n",
    "         min_rand_id = chunk_id * num_samples\n",
    "         for id in trange(min_rand_id, min_rand_id + num_samples):  \n",
    "            for i in range(2):\n",
    "                # ground truth (mask) processing\n",
    "                gt_data = h5_file[f'frame{i}_mask'][id]\n",
    "                print(f\"gt_data.shape: {gt_data.shape}\")\n",
    "                if len(gt_data.shape) == 3:\n",
    "                    gt_data = gt_data[:, :, 0]\n",
    "                assert len(gt_data.shape) == 2, \"ground truth should be 2D\"\n",
    "\n",
    "                gt_data_unique = np.unique(gt_data)[0:-2]\n",
    "                success = False\n",
    "                while not success:\n",
    "                    # resize/filter ground truth image\n",
    "                    rand_unique_id = np.random.choice(gt_data_unique)\n",
    "                    gt_data_tmp = transform.resize(\n",
    "                        gt_data == rand_unique_id,\n",
    "                        (args.img_size, args.img_size),\n",
    "                        order=0,\n",
    "                        preserve_range=True,\n",
    "                        mode=\"constant\",\n",
    "                    )\n",
    "                    gt_data_tmp = np.uint8(gt_data_tmp)\n",
    "                    if np.sum(gt_data_tmp) > 100:  # exclude tiny objects\n",
    "                        gt_data = gt_data_tmp\n",
    "                        success = True\n",
    "\n",
    "                assert np.sum(gt_data) > 100, \"ground truth should have more than 100 pixels\"\n",
    "                assert (np.max(gt_data) == 1 and np.unique(gt_data).shape[0] == 2), \"ground truth should be binary\"\n",
    "                gts.append(gt_data)\n",
    "\n",
    "                # image processing\n",
    "                image_data = h5_file[f'frame{i}_data'][id]\n",
    "                # Remove any alpha channel if present.\n",
    "                if image_data.shape[-1] > 3 and len(image_data.shape) == 3:\n",
    "                    image_data = image_data[:, :, :3]\n",
    "                # If image is grayscale, then repeat the last channel to convert to RGB\n",
    "                if len(image_data.shape) == 2:\n",
    "                    image_data = np.repeat(image_data[:, :, None], 3, axis=-1)\n",
    "\n",
    "                # nii preprocess start\n",
    "                lower_bound, upper_bound = np.percentile(image_data, 0.5), np.percentile(image_data, 99.5)\n",
    "                image_data_pre = np.clip(image_data, lower_bound, upper_bound)\n",
    "                # min-max normalize and scale\n",
    "                image_data_pre = ((image_data_pre - np.min(image_data_pre)) / (np.max(image_data_pre) - np.min(image_data_pre)) * 255.0)\n",
    "                image_data_pre[image_data == 0] = 0\n",
    "                print(image_data_pre.shape)\n",
    "                image_data_pre = transform.resize(\n",
    "                    image_data_pre,\n",
    "                    (args.img_size, args.img_size),\n",
    "                    order=3,\n",
    "                    preserve_range=True,\n",
    "                    mode=\"constant\",\n",
    "                    anti_aliasing=True,\n",
    "                )\n",
    "                image_data_pre = np.uint8(image_data_pre)\n",
    "                imgs.append(image_data_pre)\n",
    "\n",
    "                # resize image to 3*1024*1024\n",
    "                sam_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "                resize_img = sam_transform.apply_image(image_data_pre)\n",
    "                resize_img_tensor = torch.as_tensor(resize_img.transpose(2, 0, 1)).to(args.device)\n",
    "                input_image = sam_model.preprocess(resize_img_tensor[None, :, :, :])  # (1, 3, 1024, 1024)\n",
    "                assert input_image.shape == (1, 3, sam_model.image_encoder.img_size, sam_model.image_encoder.img_size), \"input image should be resized to 1024*1024\"\n",
    "                # pre-compute the image embedding\n",
    "                with torch.no_grad():\n",
    "                    embedding = sam_model.image_encoder(input_image)\n",
    "                    img_embeddings.append(embedding.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_newh5file(filename: str, num_samples: int = 1000, chunk_id: int = 0):\n",
    "    with h5py.File(f\"{filename}.h5\", 'r') as h5_file:\n",
    "         min_rand_id = chunk_id * num_samples\n",
    "\n",
    "        #  gt_data = h5_file[f'mask'][0]\n",
    "        #  # print(gt_data.shape)\n",
    "        #  print(gt_data)\n",
    "        #  print(gt_data.shape)\n",
    "        #  gt_data_unique = np.unique(gt_data)[0:-2]\n",
    "        #  print(gt_data_unique)\n",
    "\n",
    "         for id in trange(min_rand_id, min_rand_id + num_samples):\n",
    "        #  for id in range(min_rand_id, min_rand_id + num_samples):   \n",
    "            # print(id)\n",
    "            # ground truth (mask) processing\n",
    "            gt_data = h5_file[f'mask'][id]\n",
    "            # print(gt_data.shape)\n",
    "            \n",
    "            if len(gt_data.shape) == 3:\n",
    "                gt_data = gt_data[0]            \n",
    "            assert len(gt_data.shape) == 2, \"ground truth should be 2D\"\n",
    "            # print(gt_data.shape)\n",
    "            gt_data_unique = np.unique(gt_data)[0:-2]\n",
    "            # print(gt_data_unique)\n",
    "            success = False\n",
    "            # print(f'test{id}')\n",
    "            while not success:\n",
    "                # resize/filter ground truth image\n",
    "                rand_unique_id = np.random.choice(gt_data_unique)\n",
    "                \n",
    "                gt_data_tmp = transform.resize(\n",
    "                    gt_data == rand_unique_id,\n",
    "                    (args.img_size, args.img_size),\n",
    "                    order=0,\n",
    "                    preserve_range=True,\n",
    "                    mode=\"constant\",\n",
    "                )\n",
    "                gt_data_tmp = np.uint8(gt_data_tmp)\n",
    "                if np.sum(gt_data_tmp) > 100:  # exclude tiny objects\n",
    "                    gt_data = gt_data_tmp\n",
    "                    success = True\n",
    "            # print(success)\n",
    "            assert np.sum(gt_data) > 100, \"ground truth should have more than 100 pixels\"\n",
    "            assert (np.max(gt_data) == 1 and np.unique(gt_data).shape[0] == 2), \"ground truth should be binary\"\n",
    "            gts.append(gt_data)\n",
    "\n",
    "            # image processing\n",
    "            image_data = h5_file[f'data'][id]\n",
    "            # Remove any alpha channel if present.\n",
    "            image_data = image_data[0]\n",
    "            if image_data.shape[-1] > 3 and len(image_data.shape) == 3:\n",
    "                image_data = image_data[:, :, :3]\n",
    "            # If image is grayscale, then repeat the last channel to convert to RGB\n",
    "            if len(image_data.shape) == 2:\n",
    "                image_data = np.repeat(image_data[:, :, None], 3, axis=-1)\n",
    "\n",
    "            # nii preprocess start\n",
    "            lower_bound, upper_bound = np.percentile(image_data, 0.5), np.percentile(image_data, 99.5)\n",
    "            image_data_pre = np.clip(image_data, lower_bound, upper_bound)\n",
    "            # min-max normalize and scale\n",
    "            image_data_pre = ((image_data_pre - np.min(image_data_pre)) / (np.max(image_data_pre) - np.min(image_data_pre)) * 255.0)\n",
    "            image_data_pre[image_data == 0] = 0\n",
    "            # print(image_data_pre.shape)\n",
    "            image_data_pre = transform.resize(\n",
    "                image_data_pre,\n",
    "                (args.img_size, args.img_size),\n",
    "                order=3,\n",
    "                preserve_range=True,\n",
    "                mode=\"constant\",\n",
    "                anti_aliasing=True,\n",
    "            )\n",
    "            image_data_pre = np.uint8(image_data_pre)\n",
    "            imgs.append(image_data_pre)\n",
    "\n",
    "            # resize image to 3*1024*1024\n",
    "            sam_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "            resize_img = sam_transform.apply_image(image_data_pre)\n",
    "            resize_img_tensor = torch.as_tensor(resize_img.transpose(2, 0, 1)).to(args.device)\n",
    "            input_image = sam_model.preprocess(resize_img_tensor[None, :, :, :])  # (1, 3, 1024, 1024)\n",
    "            assert input_image.shape == (1, 3, sam_model.image_encoder.img_size, sam_model.image_encoder.img_size), \"input image should be resized to 1024*1024\"\n",
    "            # pre-compute the image embedding\n",
    "            with torch.no_grad():\n",
    "                embedding = sam_model.image_encoder(input_image)\n",
    "                img_embeddings.append(embedding.cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_id: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6018/8900 [39:00<43:08:09, 53.88s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "os.makedirs(args.data_folder, exist_ok=True)\n",
    "for dataset_id in range(1):\n",
    "    print(f\"dataset_id: {dataset_id}\")\n",
    "    imgs: List[np.ndarray] = []\n",
    "    gts: List[np.ndarray] = []\n",
    "    img_embeddings: List[np.ndarray] = []\n",
    "\n",
    "    # process_h5file(args.h5_file, num_samples=8900, chunk_id=dataset_id)\n",
    "    process_newh5file(args.h5_file, num_samples=8900, chunk_id=dataset_id)\n",
    "\n",
    "\n",
    "    # stack the list to array\n",
    "    np_imgs = np.stack(imgs, axis=0)  # (n, 256, 256, 3)\n",
    "    np_gts = np.stack(gts, axis=0)  # (n, 256, 256)\n",
    "    np_img_embeddings = np.stack(img_embeddings, axis=0)  # (n, 1, 256, 64, 64)\n",
    "    np.savez_compressed(\n",
    "        join(args.data_folder, f\"{args.data_name}_chunk{dataset_id:02d}.npz\"),\n",
    "        imgs=np_imgs,\n",
    "        gts=np_gts,\n",
    "        img_embeddings=np_img_embeddings,\n",
    "    )\n",
    "    imgs.clear()\n",
    "    gts.clear()\n",
    "    img_embeddings.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RobotSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
